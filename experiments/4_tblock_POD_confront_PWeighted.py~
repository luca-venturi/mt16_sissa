from dolfin import *
from RBniCS import *
import matplotlib.pyplot as plt

class Tblock(EllipticCoercivePODBase):

    def __init__(self, V, subd, bound):
        bc_list = [
            DirichletBC(V, 0.0, bound, 1),
            DirichletBC(V, 0.0, bound, 2),
            DirichletBC(V, 0.0, bound, 3),
            DirichletBC(V, 0.0, bound, 4),
            DirichletBC(V, 0.0, bound, 5),
            DirichletBC(V, 0.0, bound, 6),
            DirichletBC(V, 0.0, bound, 7),
            DirichletBC(V, 0.0, bound, 8)
        ]
        super(Tblock, self).__init__(V, bc_list)
        self.dx = Measure("dx")(subdomain_data=subd)
        self.ds = Measure("ds")(subdomain_data=bound)
        # Use the H^1 seminorm on V as norm, instead of the H^1 norm
        u = self.u
        v = self.v
        dx = self.dx
        scalar = inner(grad(u),grad(v))*dx
        self.S = assemble(scalar)
        [bc.apply(self.S) for bc in self.bc_list] # make sure to apply BCs to the inner product matrix
    
    ## Return the alpha_lower bound.
    def get_alpha_lb(self):
        return min(self.compute_theta_a())
    
    ## Set theta multiplicative terms of the affine expansion of a.
    def compute_theta_a(self):
        mu1 = self.mu[0]
    	mu2 = self.mu[1]
    	mu3 = self.mu[2]
    	mu4 = self.mu[3]
        theta_a0 = mu1
        theta_a1 = mu2
    	theta_a2 = mu3
        theta_a3 = mu4
        return (theta_a0, theta_a1, theta_a2, theta_a3)
    
    ## Set theta multiplicative terms of the affine expansion of f.
    def compute_theta_f(self):
        return (1.0,)
    
    ## Set matrices resulting from the truth discretization of a.
    def assemble_truth_a(self):
        u = self.u
        v = self.v
        dx = self.dx
        # Assemble A0
        a0 = inner(grad(u),grad(v))*dx(1) + 1e-15*inner(u,v)*dx
        A0 = assemble(a0)
        # Assemble A1
        a1 = inner(grad(u),grad(v))*dx(2) + 1e-15*inner(u,v)*dx
        A1 = assemble(a1)
    	# Assemble A2
        a2 = inner(grad(u),grad(v))*dx(3) + 1e-15*inner(u,v)*dx
        A2 = assemble(a2)
        # Assemble A3
        a3 = inner(grad(u),grad(v))*dx(4) + 1e-15*inner(u,v)*dx
        A3 = assemble(a3)
        # Return
        return (A0, A1, A2, A3)
    
    ## Set vectors resulting from the truth discretization of f.
    def assemble_truth_f(self):
        v = self.v
        dx = self.dx
        ds = self.ds
        # Assemble F0
        f0 = v*dx
        F0 = assemble(f0)
        # Return
        return (F0,)


#~~~~~~~~~~~~~~~~~~~~~~~~~     MAIN PROGRAM     ~~~~~~~~~~~~~~~~~~~~~~~~~# 

# 0.

Nmax = 10 
mean_error_u = np.zeros((Nmax,8))
mu_range = [(1.0, 3.0),(0.01,1.0),(2.0,3.0),(1.0,7.0)]
first_mu = (1.5,0.4,2.1,6.0)

mesh = Mesh("Data/4_tblock.xml")
subd = MeshFunction("size_t", mesh, "Data/4_tblock_physical_region.xml")
bound = MeshFunction("size_t", mesh, "Data/4_tblock_facet_region.xml")

V = FunctionSpace(mesh, "Lagrange", 1)

parameters.linear_algebra_backend = 'PETSc'

N_xi_train = 500
N_xi_test = 500

alpha = [(10.,10.),(10.,10.),(10.,10.),(10.,10.)]
originalDistribution = BetaDistribution(alpha)
originalWeight = BetaWeight(alpha)

# 1.1
tb_1 = Tblock(V, subd, bound)

tb_1.setmu_range(mu_range)
tb_1.setNmax(Nmax)

distribution = UniformDistribution()
density = originalWeight

tb_1.setxi_train(N_xi_train, sampling=distribution)
tb_1.set_density(weight=density)
tb_1.set_weighted_flag(1)

tb_1.setmu(first_mu)
tb_1.offline() 

print '# sampling = ', len(tb_1.xi_train)
print ''

tb_1.setxi_test(N_xi_test, enable_import=True, sampling=originalDistribution)
mean_error_u[:,1] = tb_1.error_analysis()

# 1.2
tb_2 = Tblock(V, subd, bound)

tb_2.setmu_range(mu_range)
tb_2.setNmax(Nmax)

distribution = QuadratureDistribution('ClenshawCurtis',0)
density = originalWeight

tb_2.setxi_train(N_xi_train, sampling=distribution)
tb_2.set_density(weight=density)
tb_2.set_weighted_flag(1)

tb_2.setmu(first_mu)
tb_2.offline() 

print '# sampling = ', len(tb_2.xi_train)
print '' 

tb_2.setxi_test(N_xi_test, enable_import=True, sampling=originalDistribution)
mean_error_u[:,2] = tb_2.error_analysis()

# 1.3
tb_3 = Tblock(V, subd, bound)

tb_3.setmu_range(mu_range)
tb_3.setNmax(Nmax)

distribution = QuadratureDistribution('GaussLegendre',0)
density = originalWeight

tb_3.setxi_train(N_xi_train, sampling=distribution)
tb_3.set_density(weight=density)
tb_3.set_weighted_flag(1)

tb_3.setmu(first_mu)
tb_3.offline() 

print '# sampling = ', len(tb_3.xi_train)
print '' 

tb_3.setxi_test(N_xi_test, enable_import=True, sampling=originalDistribution)
mean_error_u[:,3] = tb_3.error_analysis()

# 1.4
tb_4 = Tblock(V, subd, bound)

tb_4.setmu_range(mu_range)
tb_4.setNmax(Nmax)

distribution = QuadratureDistribution('Linspace',0)
density = originalWeight

tb_4.setxi_train(N_xi_train, sampling=distribution)
tb_4.set_density(weight=density)
tb_4.set_weighted_flag(1)

tb_4.setmu(first_mu)
tb_4.offline() 

print '# sampling = ', len(tb_4.xi_train)
print ''

tb_4.setxi_test(N_xi_test, enable_import=True, sampling=originalDistribution)
mean_error_u[:,4] = tb_4.error_analysis()

# 1.5
tb_5 = Tblock(V, subd, bound)

tb_5.setmu_range(mu_range)
tb_5.setNmax(Nmax)

distribution = QuadratureDistribution('ClenshawCurtis',1)
density = originalWeight

tb_5.setxi_train(N_xi_train, sampling=distribution)
tb_5.set_density(weight=density)
tb_5.set_weighted_flag(1)

tb_5.setmu(first_mu)
tb_5.offline()  

print '# sampling = ', len(tb_5.xi_train)
print ''

tb_5.setxi_test(N_xi_test, enable_import=True, sampling=originalDistribution)
mean_error_u[:,5] = tb_5.error_analysis()

# 1.6
tb_6 = Tblock(V, subd, bound)

tb_6.setmu_range(mu_range)
tb_6.setNmax(Nmax)

distribution = QuadratureDistribution('GaussLegendre',1)

tb_6.setxi_train(N_xi_train, sampling=distribution)
density = originalWeight

tb_6.setxi_train(N_xi_train, sampling=distribution)
tb_6.set_density(weight=density)
tb_6.set_weighted_flag(1)

tb_6.setmu(first_mu)
tb_6.offline() 

print '# sampling = ', len(tb_6.xi_train)
print '' 

tb_6.setxi_test(N_xi_test, enable_import=True, sampling=originalDistribution)
mean_error_u[:,6] = tb_6.error_analysis()

# 1.7
tb_7 = Tblock(V, subd, bound)

tb_7.setmu_range(mu_range)
tb_7.setNmax(Nmax)

distribution = QuadratureDistribution('Linspace',1)
density = originalWeight

tb_7.setxi_train(N_xi_train, sampling=distribution)
tb_7.set_density(weight=density)
tb_7.set_weighted_flag(1)

tb_7.setmu(first_mu)
tb_7.offline() 

print '# sampling = ', len(tb_7.xi_train)
print ''

tb_7.setxi_test(N_xi_test, enable_import=True, sampling=originalDistribution)
mean_error_u[:,7] = tb_7.error_analysis()

# 7. Plot the errors

plt.plot(np.log10(mean_error_u[:,1]),'b',label='wPOD - Uniform')
plt.plot(np.log10(mean_error_u[:,2]),'y',label='wPOD - Chebyshev')
plt.plot(np.log10(mean_error_u[:,3]),'k',label='wPOD - GaussLegendre')
plt.plot(np.log10(mean_error_u[:,4]),'g',label='wPOD - Linspace')
plt.plot(np.log10(mean_error_u[:,5]),'y--',label='wPOD - Chebyshev Sparse')
plt.plot(np.log10(mean_error_u[:,6]),'k--',label='wPOD - GaussLegendre Sparse')
plt.plot(np.log10(mean_error_u[:,7]),'g--',label='wPOD - Linspace Sparse')

plt.legend()
plt.show()

# 8. Returns the best method

mean_N_err = np.mean(np.log10(mean_error_u[:,1:]), axis=0) #
m = min(mean_N_err) #
print [i for i, j in enumerate(mean_N_err) if j == m] #
for i in range(1,8):
    sampling_cardinality = 'len(tb_'+str(i)+'.xi_train)'
    print eval(sampling_cardinality)

